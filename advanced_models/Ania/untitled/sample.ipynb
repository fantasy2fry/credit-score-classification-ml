{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-17T16:37:01.009530Z",
     "start_time": "2024-04-17T16:36:59.424008Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#gra\n",
    "#niech zaczyna w dolnym lewnym rogu\n",
    "start = (6,0)\n",
    "end = [(6, 6), (0, 0)]\n",
    "acts = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "eps=0.1\n",
    "\n",
    "#ruchy\n",
    "def take_action(state, action):\n",
    "    r,c = state\n",
    "\n",
    "    if action == 'UP':\n",
    "        next_state = (max(r - 1, 0), c)\n",
    "    elif action == 'DOWN':\n",
    "        next_state = (min(r + 1, 6), c)\n",
    "    elif action == 'LEFT':\n",
    "        next_state = (r, max(c - 1, 0))\n",
    "    elif action == 'RIGHT':\n",
    "        if c == 5:\n",
    "            next_state = (r, 6)\n",
    "        else:\n",
    "            next_state = (r, min(c + 2, 6))\n",
    "    reward = -1\n",
    "\n",
    "    return next_state, reward\n",
    "\n",
    "# update polityki\n",
    "def update_policy(state, policy, epsilon, Q, A=acts):\n",
    "    best_actions = [a for a in A if Q[state][a] == max(Q[state].values())]\n",
    "    policy[state] = {a: 0 for a in A}\n",
    "    for a in A:\n",
    "        if (len(A)-len(best_actions))==0:\n",
    "            policy[state][a]=1/len(A)\n",
    "        elif a in best_actions:\n",
    "            policy[state][a] = (1 - epsilon)/len(best_actions)\n",
    "        else:\n",
    "            policy[state][a] = epsilon/(len(A)-len(best_actions))\n",
    "    return policy\n",
    "\n",
    "\n",
    "#stosowanie polityki\n",
    "def choose_action(state, policy):\n",
    "    acts = list(policy[state].keys())\n",
    "    probabilities = list(policy[state].values())\n",
    "    return np.random.choice(acts, p=probabilities)\n",
    "\n",
    "# Algorytm mc\n",
    "def first_visit_mc_control(epsilon=eps, episodes=5000, A=acts):\n",
    "    policy = {}\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    count= 0 #do zliczania epizodów bez zmiany polityki\n",
    "    sum_G=0\n",
    "\n",
    "    #nie mamy informacji zadnych jeszcze, wiec random\n",
    "    for i in range(7):\n",
    "        for j in range(7):\n",
    "            state = (i, j)\n",
    "            policy[state] = {a: 0.25 for a in A}\n",
    "            Q[state] = {a: 0 for a in A}\n",
    "            returns[state] = {a: [] for a in A}\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state = start\n",
    "        states = [state]\n",
    "        rewards = [0]\n",
    "        G=0\n",
    "        actions=[]\n",
    "\n",
    "        while state not in end:\n",
    "            action = choose_action(state,policy)\n",
    "            next_state, reward = take_action(state, action)\n",
    "            states.append(next_state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "\n",
    "        for t in reversed(range(len(states) - 1)):\n",
    "            state = states[t]\n",
    "            action = actions[t]\n",
    "            reward = rewards[t + 1]\n",
    "            G = reward + G\n",
    "            if (state, action) not in zip(states[:t], actions[:t]):\n",
    "                old_policy=policy.copy()\n",
    "                returns[state][action].append(G)\n",
    "                Q[state][action] = np.mean(returns[state][action])\n",
    "                new_policy=update_policy(state, policy, epsilon, Q)\n",
    "                if new_policy!=old_policy:\n",
    "                    count=0\n",
    "                policy=new_policy\n",
    "        count+=1\n",
    "        if count==100:\n",
    "            break\n",
    "        sum_G+=G\n",
    "    return ep, sum_G\n",
    "#algorytm td\n",
    "\n",
    "def sarsa(alpha, episodes=5000, gamma=1.0, epsilon=eps):\n",
    "    count=0\n",
    "    Q = {}\n",
    "    policy = {}\n",
    "    sum_G=0\n",
    "    for i in range(7):\n",
    "        for j in range(7):\n",
    "            state = (i, j)\n",
    "            Q[state] = {a:0 if state in end else np.random.rand() for a in acts}#random dla nieterminalnych stanów\n",
    "            policy[state] = {a: 0.25 for a in acts}\n",
    "    for ep in range(episodes):\n",
    "        state = start\n",
    "        action = choose_action(state,policy)\n",
    "        while state not in end:\n",
    "            next_state, reward = take_action(state, action)\n",
    "            sum_G+=reward\n",
    "            old_policy=policy.copy()\n",
    "            new_policy=update_policy(state, policy, epsilon, Q)\n",
    "            next_action = choose_action(next_state,policy)\n",
    "            Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])\n",
    "            state = next_state\n",
    "            action=next_action\n",
    "            if new_policy!=old_policy:\n",
    "                count=0\n",
    "            policy=new_policy\n",
    "        count+=1\n",
    "        if count==100:\n",
    "            break\n",
    "            \n",
    "    return ep, sum_G"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T19:19:32.395947Z",
     "start_time": "2024-04-17T19:19:32.365903Z"
    }
   },
   "id": "59c963effcafa18a",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of episodes for Monte Carlo: 209.4, reward: -11.929799426934098\n",
      "Average number of episodes for SARSA (alpha=0.05): 4096.5, reward: -5.74761381667277\n",
      "Average number of episodes for SARSA (alpha=0.1): 2530.8, reward: -5.430930930930931\n",
      "Average number of episodes for SARSA (alpha=0.3): 1157.8, reward: -5.117809638970461\n",
      "Average number of episodes for SARSA (alpha=0.5): 3475.0, reward: -3.970043165467626\n",
      "Average number of episodes for SARSA (alpha=0.2): 1243.0, reward: -5.545454545454546\n",
      "Average number of episodes for SARSA (alpha=0.6): 4999.0, reward: -3.876015203040608\n",
      "Average number of episodes for SARSA (alpha=0.23): 1501.5, reward: -5.0651348651348655\n",
      "Average number of episodes for SARSA (alpha=0.27): 1292.2, reward: -5.055022442346386\n"
     ]
    }
   ],
   "source": [
    "# badanie zbieznosci \n",
    "avg_ep_mc=0\n",
    "avg_ep_sarsa_a1=0\n",
    "avg_ep_sarsa_a2=0\n",
    "avg_ep_sarsa_a3=0\n",
    "avg_ep_sarsa_a4=0\n",
    "avg_ep_sarsa_a5=0\n",
    "avg_ep_sarsa_a6=0\n",
    "avg_ep_sarsa_a7=0\n",
    "avg_ep_sarsa_a8=0\n",
    "reward1=0\n",
    "reward2=0\n",
    "reward3=0\n",
    "reward5=0\n",
    "reward4=0\n",
    "reward6=0\n",
    "reward7=0\n",
    "reward8=0\n",
    "reward9=0\n",
    "for i in range(10):\n",
    "    ep1, Q1=first_visit_mc_control()\n",
    "    avg_ep_mc+=ep1\n",
    "    reward1+=Q1\n",
    "    ep2, Q2=sarsa(alpha=0.05)\n",
    "    avg_ep_sarsa_a1+=ep2\n",
    "    reward2+=Q2\n",
    "    ep3, Q3=sarsa(alpha=0.1)\n",
    "    avg_ep_sarsa_a2+=ep3\n",
    "    reward3+=Q3\n",
    "    ep4, Q4=sarsa(alpha=0.3)\n",
    "    avg_ep_sarsa_a3+=ep4\n",
    "    reward4+=Q4\n",
    "    ep5, Q5=sarsa(alpha=0.5)\n",
    "    avg_ep_sarsa_a4+=ep5\n",
    "    reward5+=Q5\n",
    "    ep6, Q6=sarsa(alpha=0.2)\n",
    "    avg_ep_sarsa_a5+=ep6\n",
    "    reward6+=Q6\n",
    "    ep7, Q7=sarsa(alpha=0.6)\n",
    "    avg_ep_sarsa_a6+=ep7\n",
    "    reward7+=Q7\n",
    "    ep8, Q8=sarsa(alpha=0.23)\n",
    "    avg_ep_sarsa_a7+=ep8\n",
    "    reward8+=Q8\n",
    "    ep9, Q9=sarsa(alpha=0.27)\n",
    "    avg_ep_sarsa_a8+=ep9\n",
    "    reward9+=Q9\n",
    "    \n",
    "print(f\"Average number of episodes for Monte Carlo: {avg_ep_mc/10}, reward: {reward1/(avg_ep_mc)}\")\n",
    "print(f\"Average number of episodes for SARSA (alpha=0.05): {avg_ep_sarsa_a1/10}, reward: {reward2/(avg_ep_sarsa_a1)}\")\n",
    "print(f\"Average number of episodes for SARSA (alpha=0.1): {avg_ep_sarsa_a2/10}, reward: {reward3/(avg_ep_sarsa_a2)}\")\n",
    "print(f\"Average number of episodes for SARSA (alpha=0.3): {avg_ep_sarsa_a3/10}, reward: {reward4/(avg_ep_sarsa_a3)}\")\n",
    "print(f\"Average number of episodes for SARSA (alpha=0.5): {avg_ep_sarsa_a4/10}, reward: {reward5/(avg_ep_sarsa_a4)}\")\n",
    "print(f\"Average number of episodes for SARSA (alpha=0.2): {avg_ep_sarsa_a5/10}, reward: {reward6/(avg_ep_sarsa_a5)}\")\n",
    "print(f\"Average number of episodes for SARSA (alpha=0.6): {avg_ep_sarsa_a6/10}, reward: {reward7/(avg_ep_sarsa_a6)}\")\n",
    "print(f\"Average number of episodes for SARSA (alpha=0.23): {avg_ep_sarsa_a7/10}, reward: {reward8/(avg_ep_sarsa_a7)}\")\n",
    "print(f\"Average number of episodes for SARSA (alpha=0.27): {avg_ep_sarsa_a8/10}, reward: {reward9/(avg_ep_sarsa_a8)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T19:20:24.704576Z",
     "start_time": "2024-04-17T19:19:32.952540Z"
    }
   },
   "id": "3748a7e4f44b821f",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9fa9b46b79a0ec26"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
